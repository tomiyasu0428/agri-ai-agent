"""
ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüAgriDatabase„ÇØ„É©„Çπ
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from .database_pool import DatabasePool
from ..exceptions import DatabaseQueryError
from ..utils.error_handling import DatabaseErrorHandler

logger = logging.getLogger(__name__)


class OptimizedAgriDatabase:
    """ÊúÄÈÅ©Âåñ„Åï„Çå„ÅüAgriDatabase„ÇØ„É©„Çπ"""
    
    def __init__(self, db_pool: DatabasePool):
        self.db_pool = db_pool
        
        # „Ç≥„É¨„ÇØ„Ç∑„Éß„É≥Âêç„ÅÆÂÆöÊï∞
        self.COLLECTIONS = {
            "tasks": "‰ΩúÊ•≠Ë®àÁîª",
            "fields": "ÂúÉÂ†¥„Éá„Éº„Çø",
            "planting_plans": "‰Ωú‰ªòË®àÁîª",
            "materials": "Ë≥áÊùê„Éá„Éº„Çø",
            "material_usage": "Ë≥áÊùê‰ΩøÁî®Ë®òÈå≤",
            "crops": "‰ΩúÁâ©„Éá„Éº„Çø",
            "work_records": "‰ΩúÊ•≠Ë®òÈå≤"
        }
    
    @DatabaseErrorHandler.handle_query_error(logger)
    async def get_today_tasks(self, worker_id: str, date: str) -> List[Dict[str, Any]]:
        """‰ªäÊó•„ÅÆ„Çø„Çπ„ÇØ„ÇíÂèñÂæóÔºàÊúÄÈÅ©ÂåñÁâàÔºâ"""
        try:
            # ÈõÜÁ¥Ñ„Éë„Ç§„Éó„É©„Ç§„É≥„ÅßJOINÊìç‰Ωú„Çí‰∏ÄÂ∫¶„Å´ÂÆüË°å
            pipeline = [
                {
                    "$match": {
                        "ÊãÖÂΩìËÄÖ": worker_id,
                        "‰∫àÂÆöÊó•": date,
                        "„Çπ„ÉÜ„Éº„Çø„Çπ": {"$ne": "‚úÖ ÂÆå‰∫Ü"}
                    }
                },
                {
                    "$lookup": {
                        "from": "‰Ωú‰ªòË®àÁîª",
                        "localField": "Èñ¢ÈÄ£„Åô„Çã‰Ωú‰ªòË®àÁîª",
                        "foreignField": "‰Ωú‰ªòË®àÁîªID",
                        "as": "planting_info"
                    }
                },
                {
                    "$lookup": {
                        "from": "ÂúÉÂ†¥„Éá„Éº„Çø",
                        "localField": "planting_info.ÂúÉÂ†¥",
                        "foreignField": "ÂúÉÂ†¥ID",
                        "as": "field_info"
                    }
                },
                {
                    "$addFields": {
                        "ÂúÉÂ†¥Âêç (from ÂúÉÂ†¥„Éá„Éº„Çø) (from Èñ¢ÈÄ£„Åô„Çã‰Ωú‰ªòË®àÁîª)": {
                            "$arrayElemAt": ["$field_info.ÂúÉÂ†¥Âêç", 0]
                        },
                        "‰ΩúÁâ©Âêç": {
                            "$arrayElemAt": ["$planting_info.‰ΩúÁâ©", 0]
                        }
                    }
                },
                {
                    "$sort": {"‰∫àÂÆöÊó•": 1, "‰ΩúÊ•≠Ë®àÁîªID": 1}
                }
            ]
            
            tasks = await self.db_pool.aggregate_cached(
                self.COLLECTIONS["tasks"],
                pipeline,
                use_cache=True
            )
            
            logger.info(f"Retrieved {len(tasks)} tasks for worker {worker_id} on {date}")
            return tasks
            
        except Exception as e:
            logger.error(f"Error retrieving today's tasks: {e}")
            raise DatabaseQueryError(
                f"‰ªäÊó•„ÅÆ„Çø„Çπ„ÇØÂèñÂæó‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}",
                context={"worker_id": worker_id, "date": date}
            )
    
    @DatabaseErrorHandler.handle_query_error(logger)
    async def get_field_status(self, field_name: str) -> Optional[Dict[str, Any]]:
        """ÂúÉÂ†¥„Çπ„ÉÜ„Éº„Çø„Çπ„ÇíÂèñÂæóÔºàÊúÄÈÅ©ÂåñÁâàÔºâ"""
        try:
            # ÂúÉÂ†¥„Éá„Éº„Çø„Å®Èñ¢ÈÄ£„Åô„Çã‰Ωú‰ªòË®àÁîª„Çí‰∏ÄÂ∫¶„Å´ÂèñÂæó
            pipeline = [
                {
                    "$match": {"ÂúÉÂ†¥Âêç": field_name}
                },
                {
                    "$lookup": {
                        "from": "‰Ωú‰ªòË®àÁîª",
                        "localField": "ÂúÉÂ†¥ID",
                        "foreignField": "ÂúÉÂ†¥",
                        "as": "planting_plans"
                    }
                },
                {
                    "$lookup": {
                        "from": "Ë≥áÊùê‰ΩøÁî®Ë®òÈå≤",
                        "let": {"field_id": "$ÂúÉÂ†¥ID"},
                        "pipeline": [
                            {
                                "$match": {
                                    "$expr": {"$eq": ["$ÂúÉÂ†¥", "$$field_id"]},
                                    "‰ΩøÁî®Êó•": {
                                        "$gte": (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
                                    }
                                }
                            },
                            {"$sort": {"‰ΩøÁî®Êó•": -1}},
                            {"$limit": 5}
                        ],
                        "as": "recent_material_usage"
                    }
                }
            ]
            
            results = await self.db_pool.aggregate_cached(
                self.COLLECTIONS["fields"],
                pipeline,
                use_cache=True
            )
            
            if not results:
                logger.warning(f"Field not found: {field_name}")
                return None
            
            field_data = results[0]
            logger.info(f"Retrieved field status for: {field_name}")
            return field_data
            
        except Exception as e:
            logger.error(f"Error retrieving field status: {e}")
            raise DatabaseQueryError(
                f"ÂúÉÂ†¥„Çπ„ÉÜ„Éº„Çø„ÇπÂèñÂæó‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}",
                context={"field_name": field_name}
            )
    
    @DatabaseErrorHandler.handle_query_error(logger)
    async def get_pesticide_recommendations(
        self, 
        field_name: str, 
        crop: str
    ) -> List[Dict[str, Any]]:
        """Ëæ≤Ëñ¨Êé®Â•®„ÇíÂèñÂæóÔºàÊúÄÈÅ©ÂåñÁâàÔºâ"""
        try:
            # ‰ΩúÁâ©„Å®ÂúÉÂ†¥„Å´Âü∫„Å•„ÅÑ„Å¶Ë≥áÊùê„ÇíÊ§úÁ¥¢
            pipeline = [
                {
                    "$match": {
                        "$or": [
                            {"ÂØæË±°‰ΩúÁâ©": {"$regex": crop, "$options": "i"}},
                            {"Ë≥áÊùêÂàÜÈ°û": {"$in": ["Ëæ≤Ëñ¨", "Èò≤Èô§Ââ§", "ÊÆ∫Ëô´Ââ§", "ÊÆ∫ËèåÂâ§"]}}
                        ]
                    }
                },
                {
                    "$lookup": {
                        "from": "Ë≥áÊùê‰ΩøÁî®Ë®òÈå≤",
                        "let": {"material_name": "$Ë≥áÊùêÂêç"},
                        "pipeline": [
                            {
                                "$match": {
                                    "$expr": {"$eq": ["$Ë≥áÊùêÂêç", "$$material_name"]},
                                    "‰ΩøÁî®Êó•": {
                                        "$gte": (datetime.now() - timedelta(days=90)).strftime("%Y-%m-%d")
                                    }
                                }
                            }
                        ],
                        "as": "usage_history"
                    }
                },
                {
                    "$addFields": {
                        "recent_usage_count": {"$size": "$usage_history"},
                        "last_used": {
                            "$max": "$usage_history.‰ΩøÁî®Êó•"
                        }
                    }
                },
                {
                    "$sort": {
                        "recent_usage_count": -1,
                        "Ë≥áÊùêÂêç": 1
                    }
                },
                {
                    "$limit": 10
                }
            ]
            
            recommendations = await self.db_pool.aggregate_cached(
                self.COLLECTIONS["materials"],
                pipeline,
                use_cache=True
            )
            
            logger.info(f"Retrieved {len(recommendations)} pesticide recommendations for {crop} in {field_name}")
            return recommendations
            
        except Exception as e:
            logger.error(f"Error retrieving pesticide recommendations: {e}")
            raise DatabaseQueryError(
                f"Ëæ≤Ëñ¨Êé®Â•®ÂèñÂæó‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}",
                context={"field_name": field_name, "crop": crop}
            )
    
    @DatabaseErrorHandler.handle_query_error(logger)
    async def get_recent_material_usage(
        self, 
        field_name: str, 
        days: int = 30
    ) -> List[Dict[str, Any]]:
        """ÊúÄËøë„ÅÆË≥áÊùê‰ΩøÁî®Â±•Ê≠¥„ÇíÂèñÂæóÔºàÊúÄÈÅ©ÂåñÁâàÔºâ"""
        try:
            # ÂúÉÂ†¥ID„ÇíÂèñÂæó
            field_data = await self.db_pool.find_one_cached(
                self.COLLECTIONS["fields"],
                {"ÂúÉÂ†¥Âêç": field_name},
                {"ÂúÉÂ†¥ID": 1}
            )
            
            if not field_data:
                return []
            
            field_id = field_data.get("ÂúÉÂ†¥ID")
            start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            
            # Ë≥áÊùê‰ΩøÁî®Ë®òÈå≤„ÇíÂèñÂæó
            pipeline = [
                {
                    "$match": {
                        "ÂúÉÂ†¥": field_id,
                        "‰ΩøÁî®Êó•": {"$gte": start_date}
                    }
                },
                {
                    "$lookup": {
                        "from": "Ë≥áÊùê„Éá„Éº„Çø",
                        "localField": "Ë≥áÊùêÂêç",
                        "foreignField": "Ë≥áÊùêÂêç",
                        "as": "material_info"
                    }
                },
                {
                    "$addFields": {
                        "Ë≥áÊùêÂàÜÈ°û": {
                            "$arrayElemAt": ["$material_info.Ë≥áÊùêÂàÜÈ°û", 0]
                        }
                    }
                },
                {
                    "$sort": {"‰ΩøÁî®Êó•": -1}
                }
            ]
            
            usage_records = await self.db_pool.aggregate_cached(
                self.COLLECTIONS["material_usage"],
                pipeline,
                use_cache=True
            )
            
            logger.info(f"Retrieved {len(usage_records)} material usage records for {field_name}")
            return usage_records
            
        except Exception as e:
            logger.error(f"Error retrieving material usage: {e}")
            raise DatabaseQueryError(
                f"Ë≥áÊùê‰ΩøÁî®Â±•Ê≠¥ÂèñÂæó‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}",
                context={"field_name": field_name, "days": days}
            )
    
    @DatabaseErrorHandler.handle_query_error(logger)
    async def complete_task(
        self, 
        task_id: str, 
        completion_data: Dict[str, Any]
    ) -> bool:
        """„Çø„Çπ„ÇØ„ÇíÂÆå‰∫ÜÔºàÊúÄÈÅ©ÂåñÁâàÔºâ"""
        try:
            update_data = {
                "$set": {
                    "„Çπ„ÉÜ„Éº„Çø„Çπ": "‚úÖ ÂÆå‰∫Ü",
                    "ÂÆå‰∫ÜÊó•": datetime.now().strftime("%Y-%m-%d"),
                    **completion_data
                }
            }
            
            success = await self.db_pool.update_one(
                self.COLLECTIONS["tasks"],
                {"‰ΩúÊ•≠Ë®àÁîªID": task_id},
                update_data
            )
            
            if success:
                logger.info(f"Task completed successfully: {task_id}")
            else:
                logger.warning(f"Task not found or not updated: {task_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"Error completing task: {e}")
            raise DatabaseQueryError(
                f"„Çø„Çπ„ÇØÂÆå‰∫ÜÂá¶ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}",
                context={"task_id": task_id, "completion_data": completion_data}
            )
    
    @DatabaseErrorHandler.handle_query_error(logger)
    async def schedule_next_task(
        self, 
        field_name: str, 
        task_type: str, 
        days_offset: int
    ) -> str:
        """Ê¨°Âõû„Çø„Çπ„ÇØ„Çí„Çπ„Ç±„Ç∏„É•„Éº„É´ÔºàÊúÄÈÅ©ÂåñÁâàÔºâ"""
        try:
            # ÂúÉÂ†¥„Éá„Éº„Çø„ÇíÂèñÂæó
            field_data = await self.db_pool.find_one_cached(
                self.COLLECTIONS["fields"],
                {"ÂúÉÂ†¥Âêç": field_name},
                {"ÂúÉÂ†¥ID": 1}
            )
            
            if not field_data:
                raise DatabaseQueryError(f"ÂúÉÂ†¥„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {field_name}")
            
            # Ê¨°ÂõûÂÆüÊñΩÊó•„ÇíË®àÁÆó
            next_date = (datetime.now() + timedelta(days=days_offset)).strftime("%Y-%m-%d")
            
            # Êñ∞„Åó„ÅÑ„Çø„Çπ„ÇØ„Çí‰ΩúÊàê
            new_task = {
                "‰ΩúÊ•≠Ë®àÁîªID": f"AUTO_{field_name}_{task_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "„Çø„Çπ„ÇØÂêç": f"{task_type}({days_offset}Êó•Âæå)",
                "ÂúÉÂ†¥": field_data["ÂúÉÂ†¥ID"],
                "‰∫àÂÆöÊó•": next_date,
                "„Çπ„ÉÜ„Éº„Çø„Çπ": "üóìÔ∏è ‰∫àÂÆö",
                "‰ΩúÊàêÊó•": datetime.now().strftime("%Y-%m-%d"),
                "Ëá™ÂãïÁîüÊàê": True
            }
            
            task_id = await self.db_pool.insert_one(
                self.COLLECTIONS["tasks"],
                new_task
            )
            
            logger.info(f"Next task scheduled: {task_id} for {field_name} on {next_date}")
            return task_id
            
        except Exception as e:
            logger.error(f"Error scheduling next task: {e}")
            raise DatabaseQueryError(
                f"Ê¨°Âõû„Çø„Çπ„ÇØ„Çπ„Ç±„Ç∏„É•„Éº„É´‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}",
                context={"field_name": field_name, "task_type": task_type, "days_offset": days_offset}
            )
    
    async def get_database_stats(self) -> Dict[str, Any]:
        """„Éá„Éº„Çø„Éô„Éº„ÇπÁµ±Ë®à„ÇíÂèñÂæó"""
        try:
            stats = {}
            
            for name, collection_name in self.COLLECTIONS.items():
                try:
                    collection = await self.db_pool.get_collection(collection_name)
                    count = await collection.count_documents({})
                    stats[name] = count
                except Exception as e:
                    logger.warning(f"Failed to get stats for {collection_name}: {e}")
                    stats[name] = -1
            
            # „Ç≠„É£„ÉÉ„Ç∑„É•Áµ±Ë®à„ÇÇËøΩÂä†
            stats["cache_stats"] = self.db_pool.get_cache_stats()
            
            return stats
            
        except Exception as e:
            logger.error(f"Error retrieving database stats: {e}")
            return {"error": str(e)}
    
    async def clear_cache(self):
        """„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢"""
        self.db_pool.clear_cache()
        logger.info("Database cache cleared")